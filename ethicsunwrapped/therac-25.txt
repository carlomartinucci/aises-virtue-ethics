The Therac-25 machine was a state-of-the-art linear accelerator developed by Atomic Energy Canada Limited (AECL) and a French company, CGR, to provide radiation treatment to cancer patients. It was the most computerized and sophisticated radiation therapy machine of its time, capable of selecting multiple treatment table positions and adjusting the type and strength of energy used, all controlled by an onboard computer. AECL sold eleven Therac-25 machines that were used in the United States and Canada beginning in 1982.

Between 1985 and 1987, six accidents occurred involving significant overdoses of radiation to patients, resulting in deaths. Patients reported being "burned by the machine," which some technicians noted, but the company initially believed such incidents were impossible. The machine was recalled in 1987 for an extensive redesign of safety features, software, and mechanical interlocks. Reports to the manufacturer led to inadequate repairs and assurances that the machines were safe. Lawsuits were filed, but no thorough investigations took place. The Food and Drug Administration (FDA) later found that there was an inadequate reporting structure within the company to follow up on reported accidents.

Two earlier versions of the Therac-25 unit, the Therac-6 and Therac-20, were built from CGR's other radiation unitsâ€”Neptune and Sagittaire. These units incorporated a microcomputer to facilitate patient data entry but could operate without an onboard computer. They featured built-in safety interlocks, positioning guides, and mechanical features that prevented radiation exposure if there was a positioning problem with the patient or machine components. Some software from the Therac-20 was reused in the Therac-25. The Therac-6 and Therac-20 had excellent safety records, relying primarily on hardware for safety controls, whereas the Therac-25 relied mainly on software.

On February 6, 1987, the FDA placed a shutdown on all machines until permanent repairs could be made. Although AECL quickly stated that a "fix" was in place and the machines were now safer, this was not the case. After this incident, Leveson and Turner (1993) compiled public information from AECL, the FDA, and various regulatory agencies and concluded that there was inadequate record-keeping during the software design. The software was inadequately tested, and "patches" from earlier versions of the machine were used. The premature assumption that the problem(s) were detected and corrected was unproven. Furthermore, AECL had great difficulty reproducing the conditions under which the issues were experienced in the clinics. The FDA restructured its reporting requirements for radiation equipment after these incidents.

As computers become more ubiquitous and control increasingly significant and complex systems, people are exposed to increasing harms and risks. The issue of accountability arises when a community expects its agents to stand up for the quality of their work. Nissenbaum (1994) argues that responsibility in our computerized society is systematically undermined, which is a disservice to the community. This concern has grown with the number of critical life services controlled by computer systems in governmental, airline, and medical arenas.

According to Nissenbaum, there are four barriers to accountability: the problem of many hands, "bugs" in the system, the computer as a scapegoat, and ownership without liability. The problem of too many hands relates to the fact that many groups of people (programmers, engineers, etc.) at various levels of a company are typically involved in the creation of a computer program and have input into the final product. When something goes wrong, there is no one individual who can be clearly held responsible. It is easy for each person involved to rationalize that he or she is not responsible for the final outcome because of the small role played. This occurred with the Therac-25, which had two prominent software errors, a failed microswitch, and a reduced number of safety features compared to earlier versions of the device. The problem of bugs in the software system causing errors in machines under certain conditions has been used as a cover for careless programming, lack of testing, and lack of safety features built into the system in the Therac-25 accident. The fact that computers "always have problems with their programming" cannot be used as an excuse for overconfidence in a product, unclear/ambiguous error messages, or improper testing of individual components of the system. Another potential obstacle is ownership of proprietary software and an unwillingness to share "trade secrets" with investigators whose job it is to protect the public (Nissenbaum 1994).

The Therac-25 incident involved what has been called one of the worst computer bugs in history (Lynch 2017), though it was largely a matter of overall design issues rather than a specific coding error. Therac-25 is a glaring example of what can go wrong in a society that is heavily dependent on technology. 